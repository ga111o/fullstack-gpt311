{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOllama is an AI language model developed and released by Mistral AI in 2019. It is a variant of the more well-known GPT-2 model from OpenAI, with some key differences:\\n\\n1. Size: Ollama is significantly smaller than GPT-2, with only 7 billion parameters compared to GPT-2's 345 billion. This makes it faster and more efficient to train and deploy.\\n2. Training Data: Ollama was trained on a smaller dataset than GPT-2, specifically on English text data from the Common Crawl project. While this limits its capabilities in certain areas (e.g., it is less proficient at generating highly specialized or technical text), it also makes it more focused on everyday language use and can be more effective at understanding natural language queries.\\n3. Fine-tuning: Ollama can be fine-tuned for specific tasks, such as language translation or chatbot development, using significantly less data than GPT-2. This makes it a more practical option for small businesses or organizations with limited resources.\\n4. Accessibility: Ollama is open source and available for download and use by anyone. In contrast, GPT-2 is only available under a restrictive license that requires developers to sign a non-disclosure agreement and pay a licensing fee.\\n\\nOverall, Ollama represents an interesting alternative to GPT-2 that offers similar capabilities at a smaller scale and with less restrictive licensing.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatOllama(model = \"mistral:latest\").predict(\"tell me about Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"\"),\n",
    "    AIMessage(content=\"hello! i'm chef Paik\"),\n",
    "    HumanMessage(content=\"give me a recipe milk tea\"),\n",
    "]\n",
    "\n",
    "model = ChatOllama(model = \"mistral:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me recipe milk tea\n"
     ]
    }
   ],
   "source": [
    "template = template = PromptTemplate.from_template(\"give me recipe {food}\")\n",
    "print(template.format(food=\"milk tea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(food=\"milk tea\")\n",
    "model.predict(prompt)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"input System Prompt!\"),\n",
    "    (\"ai\", \"imput Ai Prompt!\"),\n",
    "    (\"human\", \"imput Human Prompt!\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
